---
title: "Causal_Inference"
author: "Skyler Xu, Shannon Rong"
date: "10/11/2019"
output: html_document
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}} 

<script type="text/javascript">
<!--
function toggle_visibility(id) {
  var e = document.getElementById(id); 
  if(e.style.display == 'none')
    e.style.display = 'block';
  else
    e.style.display = 'none';
}

function answer_top(name){
  var injection1 = '<a onclick=toggle_visibility("' + name + '")><b><u>Show answer</b></u></a>\n'
  document.write(injection1 + "\n")
  var injection2 = '<div id="' + name + '"style=display:none>'
  document.write(injection2 + "\n")
}

function answer_bottom() {
  document.write("</div>" + "\n")
}
//-->
</script> 

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,
                      fig.width=6,fig.height=5 ,out.width = "95%",
                      fig.align = "center",dev="CairoPNG")
par( mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
pacman::p_load("knitr","DiagrammeR","learnr","foreigh")
```

## Introduction and Fundamentals 
You must have heard that "correlation does not imply causation".    

For example, consider the following cases:    
- Did customers buy into a product because of an email campaign or would they have converted regardless of whether we did or did not run the campaign?    
- Was there any effect of promotion on the spending behavior of customers?    
- Did people with disease got better because they took treatment or would they have gotten better anyway?    

### Fundamental of Causal Inference Problems 
We take a medical experiment as the example. We begin by consiering the problem of estimating the treatment effect compared to a control group. With a binary treatment T taking on the value 0 (control) or 1 (treatment), we define **potential outcomes**, $y_i^0$ and $y_i^1$ for unit $i$ as the outcomes that would be observed under control and treatment conditions.     

#### The problem
**Counterfactual outcome**:        
For someone assigned to the treatment condition ($T_i = 1$), $y_i^1$ is observed and $y_i^0$ is unobserved *counterfactual* outcome, which represents what would have happened to the individual if assigned to control. Conversely, for control units, $y_i^0$ is observed and $y_i^1$ is counterfactual.

**Treatment effect**:    
For unit i, treatment effect is $$y_i^1 - y_i^0$$

**The fundamental problem**:        
**At most** one of these two potential outcomes, $y_i^0$ and $y_i^1$, can be observed for each unit i. But to evaluate treatment effect, we need both potential outcomes. $y_i^0$ values are "missing" for units in treatment groups and $y_i^0$ values are "missing" for units in contorl group.

#### Ways to deal with the problem
**Close substitutes**:    
This method comes with strong assumptions. It tries to measure both $y_i^0$ and $y_i^1$ on the same unit.     
For example, dividing a piece of plastic into two parts and then exposing each of them to a corrosive chemical. In this case, we assume that pieces are identical in how they would respond with and without treatment, $y_1^0 = y_2^0$ and $y_1^1 = y_2^1$. 

**Randomization and experiementation**:    
Since we cannot compare treatment and control outcomes for the same units, we try to compare them on similar units. Similarity can be attained by **randomization**, which we will discuss later. 

**Statistical adjustment**:
In some cases, randomization can be impractical or unethical. However, in observational studies, units often end up treated or not based on characteristics that are predictive of the outcome of interest. When similarity between groups is not attained, modeling or other forms of adjustment can be used to fill in the gap. 

### Randomization
Begin with the cleanest scenario, an experiment with units randomly assigned to receive treatment and control, and with the units in the study considered as a random sample from a population of interest. 

When treatments are assigned completely at random, we can think of the different treatment groups as a set of random samples from a common population. The average treatment effect of the population is estimated by
$$avg(y^1) - avg(y^0)$$
```{r}
class <- read.table(url("http://www.stat.columbia.edu/~gelman/arm/examples/electric.company/electric.dat"), header = T)
head(class)
colnames(class)[1] <- "treatment"
```

```{r}
mean(class$treated.Posttest) - mean(class$control.Posttest)
```

Equivalently, the average causal effect of the treatment corresponds to the coefficient $\theta$ in the regression, $y_i = \alpha + \theta T_i +error_i$.
```{r}
for (k in 1:4){
  subset = class[class$Grade==k,]
  print(lm(treated.Posttest ~ treatment, data = subset))
  }
```
It's not hard to observe that the treatment appears to be more effective in the low grades. 

### Observational studies
In practice, we often work with observational data, where treatments are observed rather than assigned and it's not at all reasonable to consider the observed data under different treatments as random samples from a common population.     
In an observational study, there can be systematic diﬀerences between groups of units that receive diﬀerent treatments—diﬀerences that are outside the control of the experimenter—and they can aﬀect the outcome.

```{r}
for (k in 1:4){
  ok <- class[(class$Grade == k) & (!is.na(class$Supplement.)),]
  print(lm(treated.Posttest ~ Supplement. + treated.Pretest, data = ok))
}
```
From the regressions, the uncertainties are high enough that the comparison is inconclusive except in grade 2, but the general pattern is consistent with the reasonable hypothesis that supplementing is more effective in lower grades.

### Confounding 
Consider a situation in randomized trail, the random assignment of treatment means that there should, on average, be no significant differences between the treated and untreated groups. Therefore, any difference in mortality that we observe between groups must be due to the treatment itself.     
However, in observational study, there may exist differences between the treated group and the untreated group, other than the treatment itself. So, something other than treatment that differs, then we cannot conclusively say that any difference observed is due to the treatment. Such a difference could also plausibly be due to other variables that differ between groups.     
These variables that differ between treatment groups and control groups are called **confounders** if they also influence the outcome. 

We can represent confounders in graph: for example, X is gender, T is treatment, Y is mortality.     

```{r}
library(DiagrammeR)
graph_ex <- create_graph() %>%
  add_node(label = "X") %>% 
  add_node(label = "T") %>% 
  add_edge(from = 1, to = 2) %>% 
  add_node(label = "Y") %>% 
  add_edge(from = 2, to = 3) %>% 
  add_edge(from = 1, to = 3)
```
But if gender differed between the treatment group and untreated group, and it has no association with the outcome, then gender would not be considered a confounder. 

**Identifiable estimator**:
The goal is to measure the population average causal effect:
$$\delta = E[Y^1] - E[Y^0]$$
Typically, the way we would estimate this quantity is using the conditional sample averages:
$$\hat \delta = \hat E[Y | T = 1] - \hat E[Y | T = 0]$$
with $E[Y^1]$ the expected outcome in the *hypothetical situation* where everyone was assigned to treatment, $E[Y|T=1]$ the expected outcome for all individuals in the population who are *actually assigned* to the treatment.     

However, this $\hat \delta$ is only an unbiased estimator of the true average causal effect, when the *identifiability conditions* hold:     

1. *Exchangability*: The treated and untreated individuals are exchangeable wherein the assignment of treatment does not depend on the potential outcomes.
$$Y^1, Y^0 \perp T$$    
2. *Positivity*: The probability of receiving every level of treatment is positive for every individual.    
3. *Consistency*: The treatment is defined unambiguously, i.e. that the potential outcome that corresponds to the treatment that the individual actually received is “factual”. If an individual j, recerived treatment $t$ by means $k$, then consistency means that:
$$Y_j = Y_j(t,k)\: if\: t=T_j\: no\:matter\:the\:value\:of\:k$$
    
Basically, if there are confounders present, then the first condition will be violated. In particular, the expected outcome for the individuals that were actually treated, $E[Y|T=1]$ may not be equal to the potential outcome under treatment for the entire population. 

**Measured counfounder**:
In the situation where all confounders are measured, there do exist methods for adjusting the estimates so that we can acually estimate a causal effect.     
We can assume *conditional exchangeability* to proceed the estimation.    
    
The most common methods for adjusting the estimator to eliminate the confounding:
1. Matching, restriction, and stratification (regression)     
2. Standardization, inverse-probability weighting, and G-estimation

**Unmeasured confounder**:
If there exist unmeasured confounders that may be a common cause of both the outcome and the treatment, then *it's impossible to accurately estimate the causal effect*. 

### Missing Data
One core problem associated with causal inference is missing data. It can be that some outcomes $Y_i$ is not observed or due to dropout (a unit is no longer on observation at the time the outcome should be measured).     

**Censoring**:
We call a missing outcome a *censored* observation.     
1. Left-censoring: the event of interest has already occurred before enrollment, and this is rarely encountered.
2. Right-censoring: a subject leaves the study before an event occurs or the study ends before the event has occurred. 

**Types of missing data**:
1. Missing completely at random (MCAR): missingness doesn’t depend on outcomes
$$C_i \perp Y_i$$     
2. Missing at random (MAR): missingness may depend on observed $X$ but no further on outcome
$$C_i \perp Y_i | X_i$$    
3. Missing not at random (MNAR): missingness depends on the outcome. Typically we believe that data is MNAR if there are unmeasured factors, which affect both $C_i$ and $Y_i$.    




## Matching 

## R Package 


## Regression with instrumental variables
### Why we need to use IV?
There are situations when the ignorability assumption seems inadequate because the dataset does not appear to capture all inputs that predict both the treatment and the outcomes. In this case, controlling for observed confounding covariates through regression, subclassiﬁcation, or matching will not be suﬃcient for calculating valid causal estimates because unobserved variables could be driving diﬀerences in outcomes across groups.    
When ignorability is in doubt, the method of instrumental variables (IV) can sometimes help. IV relies on several key assumtipns: *Ignorability of the instrument*, *Non zero association* between instrument and treatment variable, *Monotonicity* and *Exclusion restriction*. (for more details, please check the links in reference)    
 
### Instrumental variables
IV are a "natural experiment" that are randomly assigned to each individual and influences the outcome only through the treatment.    
IV splits the variation in treatment variable into an *exogenous* (uncorrelated with errors) and an *endogenous* (corrlated with error) part. 

### Two-stage least squares
```{r}
library(foreign)
sesame <- read.dta("sesame.dta")

fit.2a <- lm (viewcat ~ encour, data = sesame)
viewcat.hat <- fit.2a$fitted 
fit.2b <- lm (postlet ~ viewcat.hat, data = sesame)
print(fit.2a)
print(fit.2b)
```
In this example, the coeﬃcient on viewcat.hat is the estimate of the causal eﬀect of watching Sesame Street on letter recognition for those induced to watch by the experiment.This second-stage regression does not give the correct standard error, however.

### Standard errors for IV estimates 
To adjust the standard error to account for the uncertainty in both stages of the model, we then regress the outcome on predicted compliance and covariance, this time saving the predictor matrix from this second-stage regression.
```{r}
fit.3b <- lm (postlet ~ viewcat.hat+prelet+as.factor(site)+setting, x=TRUE, data = sesame)
print(fit.3b)
```
We then compute the standard deviation of the adjusted residual but with the column of predicted treatment values replaced by observed treatment values.

The final step is to compute the adjusted standard error for the two-stage regression estimate by taking the standar error from *fit.3b* and scaling by the adjusted residual standard deviation, dibided by the residual standard deviation from *fit.3b* itself.  

### Automate the previous steps
A package available in R called *sem* that has a function, *tsls()*, that automates this process, including calculating appropriate standard errors. 
```{r}
library(sem)
iv1 <- tsls (postlet ~ regular, ~ encour, data=sesame) 
print (iv1)
```


## Reference
http://www.rebeccabarter.com/blog/2017-07-05-confounding/
http://www.stat.columbia.edu/~gelman/arm/chap9.pdf
http://www.stat.columbia.edu/~gelman/arm/chap10.pdf
http://www.stats.ox.ac.uk/~mlunn/lecturenotes1.pdf



## Weighting 
The idea of weighting observations in a survey sample is based on the idea that the sample surveyed is not quite representative of the broader population. The goal is to make the sample look more like the population. To do so, you can add a larger weight to the individuals who are underrepresented in the sample and a lower weight to those who are over-represented.    
Consider a example that young males are more likely to enter treatment group and aged males are more likely to enter control group. In this case, it would make sense that comparing the outcome of these few young males in the control group with the outcome of the many young males in the treatment group serves as a fairly good estimate of the causal effect for the subgroup of young males. So we could up-weight the young males who were placed in the control group and down-weight the young males who, as expected, were placed in the treatment group.    
    
**Inverse probability weighting**:
Inverse probability weighting literally refers to weighting the outcome measures by the inverse of the probability of the individual with a given set of covariates being assigned to their treatment.    
$$With\:propensity\:score:\: p(x)=P(T=1|X=x)$$
For treated individuals, the weight is:    
$$w(x)=\frac{1}{p(x)}$$
For control individuals, the weight is:    
$$w(x) = \frac{1}{(1-p(x))}$$

**Standardized IP-weighting**:
If propensity score is close to 0, the weights we've proposed previously will end up very large. A common alternative to it is stabilized weights, which use the marginal probability of treatment instead of 1 in the numerator.    
For treated individuals, the stabilized weight:
$$w(x)=\frac{P(T=1)}{p(x)}$$
For control individuals, the stabilized weight:
$$w(x)=\frac{1-P(T=1)}{1-p(x)}$$